{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Trained Gemma Models vs Off the Shelf Gemma on Test Set with win rate\n",
    "\n",
    "This notebook benchmarks the trained Gemma models compared with an Off the Shelf Gemma using winrate on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "First, let's make sure we have all required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 06-02 18:57:47 [__init__.py:256] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from transformers import (\n",
    "    GPT2Model,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2PreTrainedModel,\n",
    "    GPT2Config,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    TextStreamer,\n",
    " AutoTokenizer\n",
    ")\n",
    "from typing import Dict, List\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset as HFDataset\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import datetime\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Benchmark Parameters\n",
    "\n",
    "Set the parameters for your benchmark run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# Ensure this path points to where your fine-tuned model was saved\n",
    "# It should contain 'adapter_config.json', 'adapter_model.safetensors', etc.\n",
    "MODEL_PATH = \"gemma_glicko_pess\" \n",
    "# The base model used for fine-tuning\n",
    "BASE_MODEL = \"unsloth/gemma-3-1b-it\" \n",
    "MAX_SEQ_LENGTH = 600\n",
    "DATASET_NAME = \"Columbia-NLP/DPO-tldr-summarisation-preferences\"\n",
    "NUM_SAMPLES_TO_GENERATE = 10 # Adjust as needed, use -1 for the whole test set\n",
    "OUTPUT_CSV = \"generated_summaries_gemma_grpo.csv\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Load Model and Tokenizer ---\n",
    "# model_base, tokenizer_base = FastModel.from_pretrained(\n",
    "#         model_name = \"gemma_glicko_base\", # Load the adapter\n",
    "#         max_seq_length = MAX_SEQ_LENGTH,\n",
    "#         load_in_4bit = False,\n",
    "#         load_in_8bit = False,\n",
    "#     )\n",
    "# model_pess, tokenizer_pess = FastModel.from_pretrained(\n",
    "#         model_name = \"gemma_glicko_pess\", # Load the adapter\n",
    "#         max_seq_length = MAX_SEQ_LENGTH,\n",
    "#         load_in_4bit = False,\n",
    "#         load_in_8bit = False,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Setup Device and Tokenizer ---\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model_base.to(device)\n",
    "# model_base.eval() # Set model to evaluation mode\n",
    "# model_pess.to(device)\n",
    "# model_pess.eval() # Set model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: Columbia-NLP/DPO-tldr-summarisation-preferences...\n",
      "Dataset loaded.\n"
     ]
    }
   ],
   "source": [
    "# --- Load Dataset ---\n",
    "test_sr = ['running','Cooking', 'books', 'jobs', 'cats', 'travel', 'Pets', 'dogs', 'offmychest', 'self', 'college', 'personalfinance']\n",
    "print(f\"Loading dataset: {DATASET_NAME}...\")\n",
    "dataset = load_dataset(DATASET_NAME)\n",
    "test_set = dataset['test']\n",
    "dataset = test_set.add_column(\"sub_reddit\", [x['subreddit'] for x in test_set['other_info']])\n",
    "df = dataset.to_pandas()\n",
    "test_df = df.loc[ df['sub_reddit'].isin(test_sr)]\n",
    "dataset = HFDataset.from_pandas(test_df)\n",
    "print(\"Dataset loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score_chosen</th>\n",
       "      <th>score_rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3555.0</td>\n",
       "      <td>3555.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       score_chosen  score_rejected\n",
       "count        3555.0          3555.0\n",
       "mean           10.0             1.0\n",
       "std             0.0             0.0\n",
       "min            10.0             1.0\n",
       "25%            10.0             1.0\n",
       "50%            10.0             1.0\n",
       "75%            10.0             1.0\n",
       "max            10.0             1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summaries(prompt_texts, model, tokenizer, max_new_tokens=53):\n",
    "    \"\"\"\n",
    "    Generates summaries for a batch of prompts using the loaded model.\n",
    "    Args:\n",
    "        prompt_texts (list of str): A list of prompts to summarize.\n",
    "        model: The loaded Hugging Face model.\n",
    "        tokenizer: The loaded Hugging Face tokenizer.\n",
    "        max_new_tokens (int): Maximum number of new tokens to generate for each summary.\n",
    "    Returns:\n",
    "        list of str: A list of generated summaries.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        prompt_texts, # Process a list of prompts\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True, # Pad to the longest sequence in the batch\n",
    "        padding_side = 'left',\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH - max_new_tokens # Make space for generated text\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculation for inference\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,      # Use sampling\n",
    "            temperature=0.1,     # Lower temperature for less randomness\n",
    "            top_p=1,           # Nucleus sampling\n",
    "            num_return_sequences=1,\n",
    "            min_length = 53,\n",
    "        )\n",
    "\n",
    "    generated_summaries = []\n",
    "    # Decode each summary in the batch\n",
    "    # outputs contains the full sequence (prompt + summary)\n",
    "    # We need to slice off the prompt part for each generated summary\n",
    "    input_ids_length = inputs.input_ids.shape[1] # Length of the tokenized input prompts (padded)\n",
    "    for i in range(outputs.shape[0]): # Iterate through each item in the batch\n",
    "        summary_ids = outputs[i][input_ids_length:]\n",
    "        summary = tokenizer.decode(summary_ids, skip_special_tokens=True)\n",
    "        generated_summaries.append(summary.strip())\n",
    "    return generated_summaries\n",
    "\n",
    "    \n",
    "def responses(path, prompts): \n",
    "    model, tokenizer = FastModel.from_pretrained(\n",
    "        model_name = path, # Load the adapter\n",
    "        max_seq_length = MAX_SEQ_LENGTH,\n",
    "        load_in_4bit = False,\n",
    "        load_in_8bit = False,\n",
    "    )\n",
    "    model.to(device)\n",
    "    model.eval() \n",
    "    answers = []\n",
    "    num = 100\n",
    "    for i in range(math.ceil(len(prompts)/num)):\n",
    "        answers+=generate_summaries(prompts[i*num:(i+1)*num], model, tokenizer)\n",
    "    return answers\n",
    "    #  answers = []\n",
    "    # num = 10\n",
    "    # l = len(prompts)//num\n",
    "    # for i in range(num):\n",
    "    #     answers += generate_summaries(prompts[i*l:(i+1*l)], model, tokenizer)\n",
    "    # return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = list(set([t[:len(t)-8] + 'Summarize the post in two sentences:' for t in dataset['prompt']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.7: Fast Gemma3 patching. Transformers: 4.51.3. vLLM: 0.8.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.999 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n",
      "==((====))==  Unsloth 2025.4.7: Fast Gemma3 patching. Transformers: 4.51.3. vLLM: 0.8.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.999 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n",
      "123.6808774471283\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # To ignore all warnings\n",
    "start = time.time()\n",
    "pess = responses(\"gemma_glicko_pess\", prompts)\n",
    "base = responses(\"gemma_glicko_base\", prompts)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('.\\n\\n**Response:**\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "u = []\n",
    "for b in base:\n",
    "    i = b.rfind('**')\n",
    "    if i != -1:\n",
    "        t.append(b[i+2:].replace(\"\\n\", \"\"))\n",
    "    else:\n",
    "        t.append(b.replace(\"\\n\", \"\"))\n",
    "for b in pess:\n",
    "    i = b.rfind('**')\n",
    "    if i != -1:\n",
    "        u.append(b[i+2:].replace(\"\\n\", \"\"))\n",
    "    else:\n",
    "        u.append(b.replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fiancÃ© signed up for a credit card through the commissary cashier, a point card, which he initially believed was a secure way to avoid affecting his credit. However, he later discovered the card was a credit card, leading to a potential problem with his credit report\n",
      "____________\n",
      "The poster has experienced a significant life change, transitioning from a sedentary lifestyle to a demanding schedule of childcare and retail work. This shift has led to a lack of motivation and a struggle to adjust to a new routine, with a body that is struggling to release tension\n",
      "____________\n",
      "The author initially received a promising interview for a non-advertised role at a dream company, which was quickly resolved with a decision to offer them a position. However, the author's subsequent lack of communication has led them to question the company's responsiveness\n",
      "____________\n",
      "The author initially struggled with paying bills due to a lack of organization. By opening a checking account for recurring expenses and automating payments, the author successfully streamlined their finances, leading to a significant reduction in spending and a better understanding of their finances.\n",
      "\n",
      "---\n",
      "\n",
      "**\n",
      "____________\n",
      "The author is experiencing a deeply troubling situation with their aunt and uncle, who have created a toxic and abusive relationship with their 13-year-old daughter, Mary. The aunt's denial and lack of accountability for her actions, coupled with Mary'\n",
      "____________\n",
      "The dog's mouth has become significantly swollen, with two lumps appearing, prompting a visit to the emergency vet. Initially, the vet recommended Rimadyl and Tramadol, but the medication hasn't alleviated the swelling, and the dog is now unable to\n",
      "____________\n",
      "The author initially had a $60 overdraft fee due to a subscription they didn't know about, which was compounded by a $12 fee and a subsequent overdraft fee.  The author is now planning to pay the debt today, and the fee'\n",
      "____________\n",
      "The author has experienced a significant improvement in their running pace, initially from a slower 7:00/km to a faster 6:10-6:20. This improvement is linked to a deliberate, prolonged run, likely influenced by weather\n",
      "____________\n",
      "The cat was diagnosed with FLUTD (crystal-induced bladder inflammation) after a vet visit, and the treatment involves a long-term diet of Royal Canin S/O. The author is considering switching to a healthier food option, but is unsure if\n",
      "____________\n",
      "The author is an 18-year-old male who has been running for a while but is currently struggling with a hip injury that has improved.  He's aiming to complete ultra marathons in the future, but is seeking advice and guidance from\n",
      "____________\n"
     ]
    }
   ],
   "source": [
    "for a in pess[:10]:\n",
    "    print(a)\n",
    "    print('____________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Prompt' : prompts, 'Base':base, 'Pessimism' : pess}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('summaries.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
