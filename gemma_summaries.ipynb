{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Trained Gemma Models vs Off the Shelf Gemma on Test Set with win rate\n",
    "\n",
    "This notebook benchmarks the trained Gemma models compared with an Off the Shelf Gemma using winrate on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "First, let's make sure we have all required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 07-03 00:11:12 [__init__.py:256] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from transformers import (\n",
    "    GPT2Model,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2PreTrainedModel,\n",
    "    GPT2Config,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    TextStreamer,\n",
    " AutoTokenizer\n",
    ")\n",
    "from typing import Dict, List\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset as HFDataset\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import datetime\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Benchmark Parameters\n",
    "\n",
    "Set the parameters for your benchmark run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# Ensure this path points to where your fine-tuned model was saved\n",
    "# It should contain 'adapter_config.json', 'adapter_model.safetensors', etc.\n",
    "MODEL_PATH = \"gemma_glicko_pess\" \n",
    "# The base model used for fine-tuning\n",
    "BASE_MODEL = \"unsloth/gemma-3-1b-it\" \n",
    "MAX_SEQ_LENGTH = 600\n",
    "DATASET_NAME = \"Columbia-NLP/DPO-tldr-summarisation-preferences\"\n",
    "NUM_SAMPLES_TO_GENERATE = 10 # Adjust as needed, use -1 for the whole test set\n",
    "OUTPUT_CSV = \"generated_summaries_gemma_grpo.csv\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Load Model and Tokenizer ---\n",
    "# model_base, tokenizer_base = FastModel.from_pretrained(\n",
    "#         model_name = \"gemma_glicko_base\", # Load the adapter\n",
    "#         max_seq_length = MAX_SEQ_LENGTH,\n",
    "#         load_in_4bit = False,\n",
    "#         load_in_8bit = False,\n",
    "#     )\n",
    "# model_pess, tokenizer_pess = FastModel.from_pretrained(\n",
    "#         model_name = \"gemma_glicko_pess\", # Load the adapter\n",
    "#         max_seq_length = MAX_SEQ_LENGTH,\n",
    "#         load_in_4bit = False,\n",
    "#         load_in_8bit = False,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Setup Device and Tokenizer ---\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model_base.to(device)\n",
    "# model_base.eval() # Set model to evaluation mode\n",
    "# model_pess.to(device)\n",
    "# model_pess.eval() # Set model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: Columbia-NLP/DPO-tldr-summarisation-preferences...\n",
      "Dataset loaded.\n"
     ]
    }
   ],
   "source": [
    "# --- Load Dataset ---\n",
    "test_sr = ['running','Cooking', 'books', 'jobs', 'cats', 'travel', 'Pets', 'dogs', 'offmychest', 'self', 'college', 'personalfinance']\n",
    "print(f\"Loading dataset: {DATASET_NAME}...\")\n",
    "dataset = load_dataset(DATASET_NAME)\n",
    "test_set = dataset['test']\n",
    "dataset = test_set.add_column(\"sub_reddit\", [x['subreddit'] for x in test_set['other_info']])\n",
    "df = dataset.to_pandas()\n",
    "test_df = df.loc[ df['sub_reddit'].isin(test_sr)]\n",
    "dataset = HFDataset.from_pandas(test_df)\n",
    "print(\"Dataset loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>chosen</th>\n",
       "      <th>rejected</th>\n",
       "      <th>messages</th>\n",
       "      <th>score_chosen</th>\n",
       "      <th>score_rejected</th>\n",
       "      <th>other_info</th>\n",
       "      <th>sub_reddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>17044c46e73997247c4780d0784be3ddaeca39552f81fe...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'chosen_note': 'clear.        ', 'id': 't3_1g...</td>\n",
       "      <td>dogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>17044c46e73997247c4780d0784be3ddaeca39552f81fe...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'chosen_note': 'clear.      ', 'id': 't3_1gyf...</td>\n",
       "      <td>dogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>17044c46e73997247c4780d0784be3ddaeca39552f81fe...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'chosen_note': 'clear.              ', 'id': ...</td>\n",
       "      <td>dogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>17044c46e73997247c4780d0784be3ddaeca39552f81fe...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'chosen_note': 'clear.        ', 'id': 't3_1g...</td>\n",
       "      <td>dogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>17044c46e73997247c4780d0784be3ddaeca39552f81fe...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'chosen_note': 'clear.', 'id': 't3_1gyf5t', '...</td>\n",
       "      <td>dogs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               prompt  \\\n",
       "29  You are an AI assistant good at summarizing re...   \n",
       "30  You are an AI assistant good at summarizing re...   \n",
       "31  You are an AI assistant good at summarizing re...   \n",
       "32  You are an AI assistant good at summarizing re...   \n",
       "33  You are an AI assistant good at summarizing re...   \n",
       "\n",
       "                                            prompt_id  \\\n",
       "29  17044c46e73997247c4780d0784be3ddaeca39552f81fe...   \n",
       "30  17044c46e73997247c4780d0784be3ddaeca39552f81fe...   \n",
       "31  17044c46e73997247c4780d0784be3ddaeca39552f81fe...   \n",
       "32  17044c46e73997247c4780d0784be3ddaeca39552f81fe...   \n",
       "33  17044c46e73997247c4780d0784be3ddaeca39552f81fe...   \n",
       "\n",
       "                                               chosen  \\\n",
       "29  [{'content': 'You are an AI assistant good at ...   \n",
       "30  [{'content': 'You are an AI assistant good at ...   \n",
       "31  [{'content': 'You are an AI assistant good at ...   \n",
       "32  [{'content': 'You are an AI assistant good at ...   \n",
       "33  [{'content': 'You are an AI assistant good at ...   \n",
       "\n",
       "                                             rejected  \\\n",
       "29  [{'content': 'You are an AI assistant good at ...   \n",
       "30  [{'content': 'You are an AI assistant good at ...   \n",
       "31  [{'content': 'You are an AI assistant good at ...   \n",
       "32  [{'content': 'You are an AI assistant good at ...   \n",
       "33  [{'content': 'You are an AI assistant good at ...   \n",
       "\n",
       "                                             messages  score_chosen  \\\n",
       "29  [{'content': 'You are an AI assistant good at ...          10.0   \n",
       "30  [{'content': 'You are an AI assistant good at ...          10.0   \n",
       "31  [{'content': 'You are an AI assistant good at ...          10.0   \n",
       "32  [{'content': 'You are an AI assistant good at ...          10.0   \n",
       "33  [{'content': 'You are an AI assistant good at ...          10.0   \n",
       "\n",
       "    score_rejected                                         other_info  \\\n",
       "29             1.0  {'chosen_note': 'clear.        ', 'id': 't3_1g...   \n",
       "30             1.0  {'chosen_note': 'clear.      ', 'id': 't3_1gyf...   \n",
       "31             1.0  {'chosen_note': 'clear.              ', 'id': ...   \n",
       "32             1.0  {'chosen_note': 'clear.        ', 'id': 't3_1g...   \n",
       "33             1.0  {'chosen_note': 'clear.', 'id': 't3_1gyf5t', '...   \n",
       "\n",
       "   sub_reddit  \n",
       "29       dogs  \n",
       "30       dogs  \n",
       "31       dogs  \n",
       "32       dogs  \n",
       "33       dogs  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summaries(prompt_texts, model, tokenizer, max_new_tokens=53):\n",
    "    \"\"\"\n",
    "    Generates summaries for a batch of prompts using the loaded model.\n",
    "    Args:\n",
    "        prompt_texts (list of str): A list of prompts to summarize.\n",
    "        model: The loaded Hugging Face model.\n",
    "        tokenizer: The loaded Hugging Face tokenizer.\n",
    "        max_new_tokens (int): Maximum number of new tokens to generate for each summary.\n",
    "    Returns:\n",
    "        list of str: A list of generated summaries.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        prompt_texts, # Process a list of prompts\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True, # Pad to the longest sequence in the batch\n",
    "        padding_side = 'left',\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH - max_new_tokens # Make space for generated text\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculation for inference\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,      # Use sampling\n",
    "            temperature=0.1,     # Lower temperature for less randomness\n",
    "            top_p=1,           # Nucleus sampling\n",
    "            num_return_sequences=1,\n",
    "            min_length = 53,\n",
    "        )\n",
    "\n",
    "    generated_summaries = []\n",
    "    # Decode each summary in the batch\n",
    "    # outputs contains the full sequence (prompt + summary)\n",
    "    # We need to slice off the prompt part for each generated summary\n",
    "    input_ids_length = inputs.input_ids.shape[1] # Length of the tokenized input prompts (padded)\n",
    "    for i in range(outputs.shape[0]): # Iterate through each item in the batch\n",
    "        summary_ids = outputs[i][input_ids_length:]\n",
    "        summary = tokenizer.decode(summary_ids, skip_special_tokens=True)\n",
    "        generated_summaries.append(summary.strip())\n",
    "    return generated_summaries\n",
    "\n",
    "    \n",
    "def responses(path, prompts): \n",
    "    model, tokenizer = FastModel.from_pretrained(\n",
    "        model_name = path, # Load the adapter\n",
    "        max_seq_length = MAX_SEQ_LENGTH,\n",
    "        load_in_4bit = False,\n",
    "        load_in_8bit = False,\n",
    "    )\n",
    "    model.to(device)\n",
    "    model.eval() \n",
    "    answers = []\n",
    "    num = 100\n",
    "    for i in range(math.ceil(len(prompts)/num)):\n",
    "        answers+=generate_summaries(prompts[i*num:(i+1)*num], model, tokenizer)\n",
    "    return answers\n",
    "    #  answers = []\n",
    "    # num = 10\n",
    "    # l = len(prompts)//num\n",
    "    # for i in range(num):\n",
    "    #     answers += generate_summaries(prompts[i*l:(i+1*l)], model, tokenizer)\n",
    "    # return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3106/93610205.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['prompt'] = [t[:len(t)-8] + 'Summarize the post in two sentences:' for t in test['prompt']]\n",
      "/tmp/ipykernel_3106/93610205.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['chosen'] = [x[1]['content'] for x in test['chosen']]\n",
      "/tmp/ipykernel_3106/93610205.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test.drop_duplicates('prompt', inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>prompt</th>\n",
       "      <th>chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29</td>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>My dogs get bored of things very easily, so I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59</td>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>Just broke up with wife of 9 years. Slight imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201</td>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>First time cooking a ribeye steak, looking for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>253</td>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>Called probation office on sister for meth, sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>259</td>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>Teenage genius is depressed having fallen shor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>45972</td>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>I'm at my first job and I don't know how to mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>48297</td>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>Keep putting myself out there and caring for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>49609</td>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>Knee pain due to poor balance, Orthopedist pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>50132</td>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>Ex-wife owes me $2,000.  I want to know how I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>50360</td>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>Met a girl online, got involved after a few mo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>307 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                             prompt  \\\n",
       "0       29  You are an AI assistant good at summarizing re...   \n",
       "1       59  You are an AI assistant good at summarizing re...   \n",
       "2      201  You are an AI assistant good at summarizing re...   \n",
       "3      253  You are an AI assistant good at summarizing re...   \n",
       "4      259  You are an AI assistant good at summarizing re...   \n",
       "..     ...                                                ...   \n",
       "302  45972  You are an AI assistant good at summarizing re...   \n",
       "303  48297  You are an AI assistant good at summarizing re...   \n",
       "304  49609  You are an AI assistant good at summarizing re...   \n",
       "305  50132  You are an AI assistant good at summarizing re...   \n",
       "306  50360  You are an AI assistant good at summarizing re...   \n",
       "\n",
       "                                                chosen  \n",
       "0    My dogs get bored of things very easily, so I ...  \n",
       "1    Just broke up with wife of 9 years. Slight imp...  \n",
       "2    First time cooking a ribeye steak, looking for...  \n",
       "3    Called probation office on sister for meth, sh...  \n",
       "4    Teenage genius is depressed having fallen shor...  \n",
       "..                                                 ...  \n",
       "302  I'm at my first job and I don't know how to mo...  \n",
       "303  Keep putting myself out there and caring for p...  \n",
       "304  Knee pain due to poor balance, Orthopedist pre...  \n",
       "305  Ex-wife owes me $2,000.  I want to know how I ...  \n",
       "306  Met a girl online, got involved after a few mo...  \n",
       "\n",
       "[307 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = test_df[['prompt', 'chosen']]\n",
    "test['prompt'] = [t[:len(t)-8] + 'Summarize the post in two sentences:' for t in test['prompt']]\n",
    "test['chosen'] = [x[1]['content'] for x in test['chosen']]\n",
    "test.drop_duplicates('prompt', inplace=True)\n",
    "test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = test['prompt'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.7: Fast Gemma3 patching. Transformers: 4.51.3. vLLM: 0.8.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.999 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n",
      "==((====))==  Unsloth 2025.4.7: Fast Gemma3 patching. Transformers: 4.51.3. vLLM: 0.8.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.999 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n",
      "111.0006492137909\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # To ignore all warnings\n",
    "start = time.time()\n",
    "pess = responses(\"gemma_glicko_pess\", prompts)\n",
    "base = responses(\"gemma_glicko_base\", prompts)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('.\\n\\n**Response:**\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "u = []\n",
    "for b in base:\n",
    "    i = b.rfind('**')\n",
    "    if i != -1:\n",
    "        t.append(b[i+2:].replace(\"\\n\", \"\"))\n",
    "    else:\n",
    "        t.append(b.replace(\"\\n\", \"\"))\n",
    "for b in pess:\n",
    "    i = b.rfind('**')\n",
    "    if i != -1:\n",
    "        u.append(b[i+2:].replace(\"\\n\", \"\"))\n",
    "    else:\n",
    "        u.append(b.replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author is experiencing a decline in their dogs' enthusiasm and happiness.  Initially, the dogs enjoyed the author's presence and games, but as the author's visits became less frequent, their excitement faded, leading to boredom and depression.  The author\n",
      "____________\n",
      "The author is experiencing a significant shift in their life, having recently experienced a breakup and a subsequent period of emotional recovery.  While initially apprehensive about the separation, the author now feels content and optimistic about the future, with plans to pursue personal goals and a renewed\n",
      "____________\n",
      "The author is a first-time cook and is hesitant to start with a 1.25\" ribeye. They've researched the process extensively but are unsure about the best method for achieving a perfect sear, and are opting for a more frequent flipping\n",
      "____________\n",
      "The author is deeply troubled by their sister's relapse into drug use and the subsequent consequences, including her arrest and the loss of custody of her niece. Despite the pain and regret, the author feels a sense of satisfaction, believing their actions were necessary to protect\n",
      "____________\n",
      "The hero's journey begins with a sudden, inexplicable ability to pass exams effortlessly, leading to a life of academic success but ultimately a descent into mental instability and social isolation. The hero's struggle highlights the challenges of balancing academic achievement with personal well-being\n",
      "____________\n",
      "The poster is a rabbit owner who has successfully trained their ferrets, now seeking advice on how to integrate them into their home and establish a routine. They are frustrated with the previous owner's lack of knowledge about ferrets and the state of the cage,\n",
      "____________\n",
      "The author is experiencing a frustrating cycle of accepting lower-paying jobs to maintain a steady income, but the lack of a stable job has led to a sense of dissatisfaction and a desire to avoid the repetitive nature of job searching.  The author feels a sense of\n",
      "____________\n",
      "The poster initially enjoyed Reddit but became frustrated with his girlfriend's reaction to his online activity, leading to a conflict and a decline in his engagement. The poster's comments were a cynical and dismissive attack on the community, suggesting it had become a place\n",
      "____________\n",
      "The narrator is experiencing a complex and unsettling situation where they've been sleeping with a friend, F, for a casual arrangement.  Despite their affection for F, the narrator is increasingly conflicted about their feelings towards her and the potential consequences of revealing this to their\n",
      "____________\n",
      "The author experienced a profoundly miserable run, characterized by stomach cramps and a significant drop in performance.  The author initially believed the sandwich and nap were the cause, but the run's severity led them to question their training and consider quitting the half marathon.\n",
      "____________\n"
     ]
    }
   ],
   "source": [
    "for a in pess[:10]:\n",
    "    print(a)\n",
    "    print('____________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "human = test['chosen'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [x[:len(x)-38] for x in prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Prompt' : prompts, 'Base':base, 'Pessimism' : pess, 'Human' : human}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('summaries.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
