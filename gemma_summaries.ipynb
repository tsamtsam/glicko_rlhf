{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Trained Gemma Models vs Off the Shelf Gemma on Test Set with win rate\n",
    "\n",
    "This notebook benchmarks the trained Gemma models compared with an Off the Shelf Gemma using winrate on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "First, let's make sure we have all required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 07-07 09:01:02 [__init__.py:256] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from transformers import (\n",
    "    GPT2Model,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2PreTrainedModel,\n",
    "    GPT2Config,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    TextStreamer,\n",
    " AutoTokenizer\n",
    ")\n",
    "from typing import Dict, List\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset as HFDataset\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import datetime\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7b19f9649890>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seed\n",
    "seed_value = 46\n",
    "torch.manual_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Benchmark Parameters\n",
    "\n",
    "Set the parameters for your benchmark run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# Ensure this path points to where your fine-tuned model was saved\n",
    "# It should contain 'adapter_config.json', 'adapter_model.safetensors', etc.\n",
    "MODEL_PATH = \"gemma_glicko_pess\" \n",
    "# The base model used for fine-tuning\n",
    "BASE_MODEL = \"unsloth/gemma-3-1b-it\" \n",
    "MAX_SEQ_LENGTH = 600\n",
    "DATASET_NAME = \"Columbia-NLP/DPO-tldr-summarisation-preferences\"\n",
    "NUM_SAMPLES_TO_GENERATE = 10 # Adjust as needed, use -1 for the whole test set\n",
    "OUTPUT_CSV = \"generated_summaries_gemma_grpo.csv\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Load Model and Tokenizer ---\n",
    "# model_base, tokenizer_base = FastModel.from_pretrained(\n",
    "#         model_name = \"gemma_glicko_base\", # Load the adapter\n",
    "#         max_seq_length = MAX_SEQ_LENGTH,\n",
    "#         load_in_4bit = False,\n",
    "#         load_in_8bit = False,\n",
    "#     )\n",
    "# model_pess, tokenizer_pess = FastModel.from_pretrained(\n",
    "#         model_name = \"gemma_glicko_pess\", # Load the adapter\n",
    "#         max_seq_length = MAX_SEQ_LENGTH,\n",
    "#         load_in_4bit = False,\n",
    "#         load_in_8bit = False,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Setup Device and Tokenizer ---\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model_base.to(device)\n",
    "# model_base.eval() # Set model to evaluation mode\n",
    "# model_pess.to(device)\n",
    "# model_pess.eval() # Set model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: Columbia-NLP/DPO-tldr-summarisation-preferences...\n",
      "Dataset loaded.\n"
     ]
    }
   ],
   "source": [
    "# --- Load Dataset ---\n",
    "test_sr = ['running','Cooking', 'books', 'jobs', 'cats', 'travel', 'Pets', 'dogs', 'offmychest', 'self', 'college', 'personalfinance']\n",
    "print(f\"Loading dataset: {DATASET_NAME}...\")\n",
    "dataset = load_dataset(DATASET_NAME)\n",
    "test_set = dataset['test']\n",
    "dataset = test_set.add_column(\"sub_reddit\", [x['subreddit'] for x in test_set['other_info']])\n",
    "df = dataset.to_pandas()\n",
    "test_df = df.loc[ df['sub_reddit'].isin(test_sr)]\n",
    "dataset = HFDataset.from_pandas(test_df)\n",
    "print(\"Dataset loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>chosen</th>\n",
       "      <th>rejected</th>\n",
       "      <th>messages</th>\n",
       "      <th>score_chosen</th>\n",
       "      <th>score_rejected</th>\n",
       "      <th>other_info</th>\n",
       "      <th>sub_reddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>17044c46e73997247c4780d0784be3ddaeca39552f81fe...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'chosen_note': 'clear.        ', 'id': 't3_1g...</td>\n",
       "      <td>dogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>17044c46e73997247c4780d0784be3ddaeca39552f81fe...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'chosen_note': 'clear.      ', 'id': 't3_1gyf...</td>\n",
       "      <td>dogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>17044c46e73997247c4780d0784be3ddaeca39552f81fe...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'chosen_note': 'clear.              ', 'id': ...</td>\n",
       "      <td>dogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>17044c46e73997247c4780d0784be3ddaeca39552f81fe...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'chosen_note': 'clear.        ', 'id': 't3_1g...</td>\n",
       "      <td>dogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>17044c46e73997247c4780d0784be3ddaeca39552f81fe...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>[{'content': 'You are an AI assistant good at ...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'chosen_note': 'clear.', 'id': 't3_1gyf5t', '...</td>\n",
       "      <td>dogs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               prompt  \\\n",
       "29  You are an AI assistant good at summarizing re...   \n",
       "30  You are an AI assistant good at summarizing re...   \n",
       "31  You are an AI assistant good at summarizing re...   \n",
       "32  You are an AI assistant good at summarizing re...   \n",
       "33  You are an AI assistant good at summarizing re...   \n",
       "\n",
       "                                            prompt_id  \\\n",
       "29  17044c46e73997247c4780d0784be3ddaeca39552f81fe...   \n",
       "30  17044c46e73997247c4780d0784be3ddaeca39552f81fe...   \n",
       "31  17044c46e73997247c4780d0784be3ddaeca39552f81fe...   \n",
       "32  17044c46e73997247c4780d0784be3ddaeca39552f81fe...   \n",
       "33  17044c46e73997247c4780d0784be3ddaeca39552f81fe...   \n",
       "\n",
       "                                               chosen  \\\n",
       "29  [{'content': 'You are an AI assistant good at ...   \n",
       "30  [{'content': 'You are an AI assistant good at ...   \n",
       "31  [{'content': 'You are an AI assistant good at ...   \n",
       "32  [{'content': 'You are an AI assistant good at ...   \n",
       "33  [{'content': 'You are an AI assistant good at ...   \n",
       "\n",
       "                                             rejected  \\\n",
       "29  [{'content': 'You are an AI assistant good at ...   \n",
       "30  [{'content': 'You are an AI assistant good at ...   \n",
       "31  [{'content': 'You are an AI assistant good at ...   \n",
       "32  [{'content': 'You are an AI assistant good at ...   \n",
       "33  [{'content': 'You are an AI assistant good at ...   \n",
       "\n",
       "                                             messages  score_chosen  \\\n",
       "29  [{'content': 'You are an AI assistant good at ...          10.0   \n",
       "30  [{'content': 'You are an AI assistant good at ...          10.0   \n",
       "31  [{'content': 'You are an AI assistant good at ...          10.0   \n",
       "32  [{'content': 'You are an AI assistant good at ...          10.0   \n",
       "33  [{'content': 'You are an AI assistant good at ...          10.0   \n",
       "\n",
       "    score_rejected                                         other_info  \\\n",
       "29             1.0  {'chosen_note': 'clear.        ', 'id': 't3_1g...   \n",
       "30             1.0  {'chosen_note': 'clear.      ', 'id': 't3_1gyf...   \n",
       "31             1.0  {'chosen_note': 'clear.              ', 'id': ...   \n",
       "32             1.0  {'chosen_note': 'clear.        ', 'id': 't3_1g...   \n",
       "33             1.0  {'chosen_note': 'clear.', 'id': 't3_1gyf5t', '...   \n",
       "\n",
       "   sub_reddit  \n",
       "29       dogs  \n",
       "30       dogs  \n",
       "31       dogs  \n",
       "32       dogs  \n",
       "33       dogs  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summaries(prompt_texts, model, tokenizer, max_new_tokens=53):\n",
    "    \"\"\"\n",
    "    Generates summaries for a batch of prompts using the loaded model.\n",
    "    Args:\n",
    "        prompt_texts (list of str): A list of prompts to summarize.\n",
    "        model: The loaded Hugging Face model.\n",
    "        tokenizer: The loaded Hugging Face tokenizer.\n",
    "        max_new_tokens (int): Maximum number of new tokens to generate for each summary.\n",
    "    Returns:\n",
    "        list of str: A list of generated summaries.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        prompt_texts, # Process a list of prompts\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True, # Pad to the longest sequence in the batch\n",
    "        padding_side = 'left',\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH - max_new_tokens # Make space for generated text\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculation for inference\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,      # Use sampling\n",
    "            temperature=0.1,     # Lower temperature for less randomness\n",
    "            top_p=1,           # Nucleus sampling\n",
    "            num_return_sequences=1,\n",
    "            min_length = 53,\n",
    "        )\n",
    "\n",
    "    generated_summaries = []\n",
    "    # Decode each summary in the batch\n",
    "    # outputs contains the full sequence (prompt + summary)\n",
    "    # We need to slice off the prompt part for each generated summary\n",
    "    input_ids_length = inputs.input_ids.shape[1] # Length of the tokenized input prompts (padded)\n",
    "    for i in range(outputs.shape[0]): # Iterate through each item in the batch\n",
    "        summary_ids = outputs[i][input_ids_length:]\n",
    "        summary = tokenizer.decode(summary_ids, skip_special_tokens=True)\n",
    "        generated_summaries.append(summary.strip())\n",
    "    return generated_summaries\n",
    "\n",
    "    \n",
    "def responses(path, prompts): \n",
    "    model, tokenizer = FastModel.from_pretrained(\n",
    "        model_name = path, # Load the adapter\n",
    "        max_seq_length = MAX_SEQ_LENGTH,\n",
    "        load_in_4bit = False,\n",
    "        load_in_8bit = False,\n",
    "    )\n",
    "    model.to(device)\n",
    "    model.eval() \n",
    "    answers = []\n",
    "    num = 100\n",
    "    for i in range(math.ceil(len(prompts)/num)):\n",
    "        answers+=generate_summaries(prompts[i*num:(i+1)*num], model, tokenizer)\n",
    "    return answers\n",
    "    #  answers = []\n",
    "    # num = 10\n",
    "    # l = len(prompts)//num\n",
    "    # for i in range(num):\n",
    "    #     answers += generate_summaries(prompts[i*l:(i+1*l)], model, tokenizer)\n",
    "    # return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>prompt</th>\n",
       "      <th>chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33673</td>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>Currently a graphic design major, not as passi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16321</td>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>fiancÃ© accidentally signed up for a credit car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45601</td>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>I've been able to read conversations going bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22159</td>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>Need a family lawyer's advice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34224</td>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>My wife is driving like a maniac and I'm not s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>32074</td>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>Company got bought out, new contract is insane...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>18270</td>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>my neighbors are too loud all the time and it'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>23555</td>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>my cousin's parental rights were removed for d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>13555</td>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>Bought a puppy from a puppy mill. I feel stupi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>33661</td>\n",
       "      <td>You are an AI assistant good at summarizing re...</td>\n",
       "      <td>I'm underemployed and I'm wondering if anyone ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                             prompt  \\\n",
       "0    33673  You are an AI assistant good at summarizing re...   \n",
       "1    16321  You are an AI assistant good at summarizing re...   \n",
       "2    45601  You are an AI assistant good at summarizing re...   \n",
       "3    22159  You are an AI assistant good at summarizing re...   \n",
       "4    34224  You are an AI assistant good at summarizing re...   \n",
       "..     ...                                                ...   \n",
       "495  32074  You are an AI assistant good at summarizing re...   \n",
       "496  18270  You are an AI assistant good at summarizing re...   \n",
       "497  23555  You are an AI assistant good at summarizing re...   \n",
       "498  13555  You are an AI assistant good at summarizing re...   \n",
       "499  33661  You are an AI assistant good at summarizing re...   \n",
       "\n",
       "                                                chosen  \n",
       "0    Currently a graphic design major, not as passi...  \n",
       "1    fiancÃ© accidentally signed up for a credit car...  \n",
       "2    I've been able to read conversations going bac...  \n",
       "3                     Need a family lawyer's advice...  \n",
       "4    My wife is driving like a maniac and I'm not s...  \n",
       "..                                                 ...  \n",
       "495  Company got bought out, new contract is insane...  \n",
       "496  my neighbors are too loud all the time and it'...  \n",
       "497  my cousin's parental rights were removed for d...  \n",
       "498  Bought a puppy from a puppy mill. I feel stupi...  \n",
       "499  I'm underemployed and I'm wondering if anyone ...  \n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample Entries for the test set\n",
    "n = 500 \n",
    "test = test_df.sample(n)\n",
    "test = test[['prompt', 'chosen']]\n",
    "test['prompt'] = [t[:len(t)-8] + 'Summarize the post in two sentences:' for t in test['prompt']]\n",
    "test['chosen'] = [x[1]['content'] for x in test['chosen']]\n",
    "test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = test_df[['prompt', 'chosen']]\n",
    "# test['prompt'] = [t[:len(t)-8] + 'Summarize the post in two sentences:' for t in test['prompt']]\n",
    "# test['chosen'] = [x[1]['content'] for x in test['chosen']]\n",
    "# test.drop_duplicates('prompt', inplace=True)\n",
    "# test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = test['prompt'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.7: Fast Gemma3 patching. Transformers: 4.51.3. vLLM: 0.8.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.999 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n",
      "==((====))==  Unsloth 2025.4.7: Fast Gemma3 patching. Transformers: 4.51.3. vLLM: 0.8.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.999 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n",
      "165.81894612312317\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # To ignore all warnings\n",
    "start = time.time()\n",
    "pess = responses(\"gemma_glicko_pess\", prompts)\n",
    "base = responses(\"gemma_glicko_base\", prompts)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('.\\n\\n**Response:**\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "u = []\n",
    "for b in base:\n",
    "    i = b.rfind('**')\n",
    "    if i != -1:\n",
    "        t.append(b[i+2:].replace(\"\\n\", \"\"))\n",
    "    else:\n",
    "        t.append(b.replace(\"\\n\", \"\"))\n",
    "for b in pess:\n",
    "    i = b.rfind('**')\n",
    "    if i != -1:\n",
    "        u.append(b[i+2:].replace(\"\\n\", \"\"))\n",
    "    else:\n",
    "        u.append(b.replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author is a sophomore graphic design major who initially had a strong interest in graphic design but has now realized it's not a fulfilling career path. They are hesitant to switch majors but are exploring a plan to broaden their education by taking general education courses to explore\n",
      "____________\n",
      "The fiancÃ© signed up for a points card at the commissary, a credit card, and the signup was a mistake.  He initially believed it was a points card, but the credit card was revealed, leading to a potential credit card issue and a need to dispute\n",
      "____________\n",
      "The author is concerned about a potential relationship with their father, and has been reading his iMessage conversations, leading to a sense of unease and a fear of being monitored.  The author feels a lack of trust and a strained relationship with their parents, making\n",
      "____________\n",
      "The author is a 15-year-old sibling with a strained relationship with their parents and a deeply fractured family dynamic. They are seeking a custody arrangement where they are separated from their siblings, prioritizing their own well-being and a desire to avoid the\n",
      "____________\n",
      "The author is experiencing a persistent and escalating problem with their wife's driving habits, which are a significant safety risk to everyone on the road.  Despite their efforts to address the issue, the wife's behavior has consistently worsened, leading to a complete disregard\n",
      "____________\n",
      "The author is a sixteen-year-old in Seattle, Washington, seeking to visit their father, who is currently ill and facing a potentially terminal illness.  Despite his parents' concerns about the cost of the bus travel, the author is determined to make the\n",
      "____________\n",
      "The post describes a concerning situation with Cocoa and Roxy, where Cocoa's intense licking and nibbling of Roxy's tongue is a significant problem. Initially, the couple had a positive relationship with the collie/border collie, but the recent introduction of Cocoa has\n",
      "____________\n",
      "**The poster is experiencing a significant level of anxiety and fear about a partner's HIV status, despite the partner's apparent honesty and a desire to maintain a relationship. The poster's fear stems from a perceived risk of contracting AIDS, fueled by the\n",
      "____________\n",
      "The author has a long-standing, consistent relationship with a 60+ year old regular, Bill, who frequently acts entitled and aggressive towards the author.  Despite his age, Bill exhibits a lack of maturity and frequently berates the author for simply enjoying\n",
      "____________\n",
      "This individual is a young professional with a substantial income but a lack of financial discipline.  They've experienced a positive career change and are planning a significant travel and lifestyle change, but their current spending habits are unsustainable, leading to a low monthly budget of Â£\n",
      "____________\n"
     ]
    }
   ],
   "source": [
    "for a in pess[:10]:\n",
    "    print(a)\n",
    "    print('____________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "human = test['chosen'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [x[:len(x)-38] for x in prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Prompt' : prompts, 'Base':base, 'Pessimism' : pess, 'Human' : human}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('summaries' + str(seed_value) +'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
