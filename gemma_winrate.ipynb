{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c215fd8b-9481-44d1-92c7-e1a889033a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 05-28 20:45:44 [__init__.py:256] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from transformers import (\n",
    "    GPT2Model,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2PreTrainedModel,\n",
    "    GPT2Config,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    TextStreamer,\n",
    " AutoTokenizer\n",
    ")\n",
    "from typing import Dict, List\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset as HFDataset\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import datetime\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "657a9c7e-7896-4dcf-9253-a7840b77e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('summaries.csv')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d915f5a-bcb8-425f-b97d-c0691ec256ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#String processing\n",
    "def remove_before_char(text, char):\n",
    "  \"\"\"Removes the portion of the string before the first occurrence of the specified character.\n",
    "\n",
    "  Args:\n",
    "    text: The input string.\n",
    "    char: The character to search for.\n",
    "\n",
    "  Returns:\n",
    "    The modified string, or the original string if the character is not found.\n",
    "  \"\"\"\n",
    "  index = text.find(char)\n",
    "  if index != -1:\n",
    "    return text[index:]\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6c6d6cb-e46d-446c-9b8d-f00255989fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Data for GPT2\n",
    "base = []\n",
    "pess = []\n",
    "ml = 0 \n",
    "for index, row in df.iterrows():\n",
    "    # Process the prompt\n",
    "    prompt = row['Prompt']\n",
    "    index = prompt.find('Post')\n",
    "    if index != -1: #check if substring exists\n",
    "        prompt = prompt[index+5:]\n",
    "        prompt = prompt[: len(prompt)-8]\n",
    "    \n",
    "    pre = '''Which of the following summaries does a better job of summarizing the most important points in the given forum post, without including unimportant or irrelevant details? Judge based on accuracy, coverage, and coherence. \n",
    "    ### Post: {\n",
    "    ''' + prompt +  '''}\n",
    "    ### Summary A: {''' + row['Human'] + '''} \n",
    "    ### Summary B: {'''\n",
    "    post = '''} \n",
    "    \n",
    "    ### Instructions: \n",
    "    FIRST provide a one-sentence comparison of the two summaries, explaining which you prefer and why. \n",
    "    SECOND, on a new line, state only ‚ÄúA‚Äù or ‚ÄúB‚Äù to indicate your choice. Your response should use the format: \n",
    "    Comparison: <one-sentence comparison and explanation >\n",
    "    Preferred: <‚ÄúA‚Äù or ‚ÄúB‚Äù>\n",
    "    '''\n",
    "    base.append(pre + row['Base'] + post)\n",
    "    pess.append(pre + row['Pessimism'] + post)\n",
    "    ml = max(ml, len(base[-1].split(' ')), len(pess[-1].split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf9e60c-fd63-4562-858b-fa2ce98ab591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load GPT-2 Large model and tokenizer\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\n",
    "# model = GPT2LMHeadModel.from_pretrained('gpt2-large')\n",
    "# # Add padding token if missing\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29bf21df-fd51-4fd8-a926-8fe99969850d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "607"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4cd8c9dc-cf1b-4000-91cb-3e2955ae2554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.7: Fast Gemma3 patching. Transformers: 4.51.3. vLLM: 0.8.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.999 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    }
   ],
   "source": [
    "# Load in Gemma3 to evaluate if the generated summary is better than the human one\n",
    "max_seq_length = ml+ 16\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-1b-it\",\n",
    "    max_seq_length = ml, # Choose any for long context!\n",
    "    load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "54877b07-9ad5-426d-865a-7991d82b8946",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [base[0], pess[0]]\n",
    "max_new_tokens = 16\n",
    "inputs = tokenizer(\n",
    "    text, # Process a list of prompts\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True, # Pad to the longest sequence in the batch\n",
    "    truncation=True,\n",
    "    max_length=max_seq_length - max_new_tokens # Make space for generated text\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad(): # Disable gradient calculation for inference\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        do_sample=True,      # Use sampling\n",
    "        temperature=0.1,     # Lower temperature for less randomness\n",
    "        top_p=1,           # Nucleus sampling\n",
    "        num_return_sequences=1,\n",
    "        min_length = 53,\n",
    "    )\n",
    "generated_summaries = []\n",
    "# Decode each summary in the batch\n",
    "# outputs contains the full sequence (prompt + summary)\n",
    "# We need to slice off the prompt part for each generated summary\n",
    "input_ids_length = inputs.input_ids.shape[1] # Length of the tokenized input prompts (padded)\n",
    "for i in range(outputs.shape[0]): # Iterate through each item in the batch\n",
    "    summary_ids = outputs[i][input_ids_length:]\n",
    "    summary = tokenizer.decode(summary_ids, skip_special_tokens=True)\n",
    "    generated_summaries.append(summary.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "40fbe083-300d-4211-990a-cacbda61d93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indicate your preference.\n",
      "\n",
      "    Comparison: Summary A is more concise and focuses on\n"
     ]
    }
   ],
   "source": [
    "print(generated_summaries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "308bf0f5-1559-4a34-a537-86ecab68f758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indicate your choice.\n",
      "    \n",
      "    **Comparison:** Summary A is more concise\n"
     ]
    }
   ],
   "source": [
    "print(generated_summaries[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4174000c-4c5a-4685-b3e3-43dbde9a8924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"My past relationship ended because of a combination of factors, including the girl's insistence on having a boyfriend, her refusal to spend time with me, and her own desire for a relationship with her ex-boyfriend. \\n\\nIt was revealed that I was overly\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['Pessimism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943d6d9f-f61e-469c-97ef-3627095cc5af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
