{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c215fd8b-9481-44d1-92c7-e1a889033a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 05-29 23:19:40 [__init__.py:256] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from transformers import (\n",
    "    GPT2Model,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2PreTrainedModel,\n",
    "    GPT2Config,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    TextStreamer,\n",
    " AutoTokenizer\n",
    ")\n",
    "from typing import Dict, List\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset as HFDataset\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import datetime\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "657a9c7e-7896-4dcf-9253-a7840b77e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('summaries.csv')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d915f5a-bcb8-425f-b97d-c0691ec256ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#String processing\n",
    "def remove_before_char(text, char):\n",
    "  \"\"\"Removes the portion of the string before the first occurrence of the specified character.\n",
    "\n",
    "  Args:\n",
    "    text: The input string.\n",
    "    char: The character to search for.\n",
    "\n",
    "  Returns:\n",
    "    The modified string, or the original string if the character is not found.\n",
    "  \"\"\"\n",
    "  index = text.find(char)\n",
    "  if index != -1:\n",
    "    return text[index:]\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6c6d6cb-e46d-446c-9b8d-f00255989fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Data for GPT2\n",
    "base = []\n",
    "pess = []\n",
    "ml = 0 \n",
    "for index, row in df.iterrows():\n",
    "    # Process the prompt\n",
    "    prompt = row['Prompt']\n",
    "    index = prompt.find('Post')\n",
    "    if index != -1: #check if substring exists\n",
    "        prompt = prompt[index+5:]\n",
    "        prompt = prompt[: len(prompt)-8]\n",
    "    \n",
    "    pre = '''Which of the following summaries does a better job of summarizing the most important points in the given forum post, without including unimportant or irrelevant details? Judge based on accuracy, coverage, and coherence.\n",
    "    Post: ''' + prompt +  ''' \n",
    "    Summary A: ''' + row['Human'] + ''' \n",
    "    Summary B: ''' \n",
    "    \n",
    "    post = '''\n",
    "    Which summary is better? Respond with only <A> or <B>.\n",
    "    _____________________________________________________________________________________________________________'''\n",
    "    base.append(pre + row['Base'] + post)\n",
    "    pess.append(pre + row['Pessimism'] + post)\n",
    "    ml = max(ml, len(base[-1].split(' ')), len(pess[-1].split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf6bc073-da83-4664-9993-559bf8ba79ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Data for GPT2\n",
    "text = []\n",
    "ml = 0 \n",
    "for index, row in df.iterrows():\n",
    "    # Process the prompt\n",
    "    prompt = row['Prompt']\n",
    "    index = prompt.find('Post')\n",
    "    if index != -1: #check if substring exists\n",
    "        prompt = prompt[index+5:]\n",
    "        prompt = prompt[: len(prompt)-8]\n",
    "    \n",
    "    pre = '''Which of the following summaries does a better job of summarizing the most important points in the given forum post, without including unimportant or irrelevant details? Judge based on accuracy, coverage, and coherence.\n",
    "    Post: {''' + prompt +  '''} \n",
    "    Summary A: {''' + row['Base'] + '''} \n",
    "    \n",
    "    Summary B: {''' + row['Pessimism'] + '''}\n",
    "\n",
    "    \n",
    "    Which summary is better? Respond with only <A> or <B>.\n",
    "    -------------------------------------------------------\n",
    "    '''\n",
    "    text.append(pre)\n",
    "    ml = max(ml, len(text[-1].split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaf9e60c-fd63-4562-858b-fa2ce98ab591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load GPT-2 Large model and tokenizer\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\n",
    "# model = GPT2LMHeadModel.from_pretrained('gpt2-large')\n",
    "# # Add padding token if missing\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29bf21df-fd51-4fd8-a926-8fe99969850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 8\n",
    "max_seq_length = 800 + max_new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cd8c9dc-cf1b-4000-91cb-3e2955ae2554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.7: Fast Gemma3 patching. Transformers: 4.51.3. vLLM: 0.8.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.999 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gemma3ForCausalLM(\n",
       "  (model): Gemma3TextModel(\n",
       "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 1152, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear(in_features=1152, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1152, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "          (up_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "          (down_proj): Linear(in_features=6912, out_features=1152, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "    (rotary_emb): Gemma3RotaryEmbedding()\n",
       "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1152, out_features=262144, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in Gemma3 to evaluate if the generated summary is better than the human one\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-1b-it\",\n",
    "    max_seq_length = max_seq_length, # Choose any for long context!\n",
    "    load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54877b07-9ad5-426d-865a-7991d82b8946",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = base[:5] + pess[:5]\n",
    "inputs = tokenizer(\n",
    "    text, # Process a list of prompts\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True, # Pad to the longest sequence in the batch\n",
    "    truncation=True,\n",
    "    max_length=max_seq_length - max_new_tokens # Make space for generated text\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad(): # Disable gradient calculation for inference\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        do_sample=True,      # Use sampling\n",
    "        temperature=0.25,     # Lower temperature for less randomness\n",
    "        top_p=.9,           # Nucleus sampling\n",
    "        num_return_sequences=1,\n",
    "        min_length = 1,\n",
    "    )\n",
    "generated_summaries = []\n",
    "# Decode each summary in the batch\n",
    "# outputs contains the full sequence (prompt + summary)\n",
    "# We need to slice off the prompt part for each generated summary\n",
    "input_ids_length = inputs.input_ids.shape[1] # Length of the tokenized input prompts (padded)\n",
    "for i in range(outputs.shape[0]): # Iterate through each item in the batch\n",
    "    \n",
    "    summary_ids = outputs[i][input_ids_length:]\n",
    "    summary = tokenizer.decode(summary_ids, skip_special_tokens=True)\n",
    "    generated_summaries.append(summary.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65f8cf4a-f702-46e4-a5d3-5e8035cf9490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_first(text):\n",
    "    one_index = text.find(\"A\")\n",
    "    two_index = text.find(\"B\")\n",
    "\n",
    "    if one_index == -1 and two_index == -1:\n",
    "        return -1\n",
    "    elif one_index == -1:\n",
    "        return 2\n",
    "    elif two_index == -1:\n",
    "        return 1\n",
    "    elif one_index < two_index:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7d9208a-f308-4821-9be7-7d7a32c9891b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for a in generated_summaries:\n",
    "    print(check_first(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d33db24f-5501-4a04-9b43-87e4b0b46691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<A> My husband is lazy', '<A> <B>', '<A>\\n<B>', '<A>\\n<B>', '<A> <B>', '<A>\\n<B>', '<A>\\n<B>', '<A> is better.', '<A>\\n    <', '<A>\\n<B>']\n"
     ]
    }
   ],
   "source": [
    "print(generated_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "943d6d9f-f61e-469c-97ef-3627095cc5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = base[:100] + pess[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aceb75b5-fc8e-4776-95cd-ba6c6ed2a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../whome/Downloads/summ.txt', 'w') as f:\n",
    "    for line in text[:100]:\n",
    "        f.write(\"%s\\n\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5ceefba-57e4-4298-890d-5da291e1db2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which of the following summaries does a better job of summarizing the most important points in the given forum post, without including unimportant or irrelevant details? Judge based on accuracy, coverage, and coherence.\n",
      "    Post: {\n",
      "This may be obvious to some, but it has helped me get a much better picture of my finances. For years I struggled with paying bills, sometimes because of low income but just as often because I was disorganized. After making simple changes to my bank account structures I don't miss bills, have lowered my spending and can understand my expenses better.\n",
      "\n",
      "The trick is pretty simple. Open a checking account for all your reoccurring expenses. Then go about setting them all on auto-pay. Most of the time you can connect the bank numbers or use a debit card to automate payment. If you need to pay an individual or have to pay by check most banks have a bill pay feature that will send scheduled payments by check.\n",
      "\n",
      "At first you'll need to overpay into this account because your expenses estimate might be off and you need a buffered balance. After a couple of months you'll be able to pay in almost exactly what you need. Then you can setup automatic deposit to cover all your expenses.\n",
      "\n",
      "Congratulations, your financial life is now completely automated. Did you remember to pay such and such bill? Yeah totally. Analyzing your accounts it looks like you're paying for a lot of online storage. Google sells space cheap. Consolidate accounts and save. Get hit with an unexpected bill? There's plenty of buffer in the account, just pay a bit more in to make up and move on.\n",
      "\n",
      "It can take a while to move all your accounts over, but once you do the benefits of organization are massive. Also, once you make that one or two payments into the account per month (pay into it right when you get income) whatever remains is your spending / saving money.\n",
      "\n",
      "} \n",
      "    Summary A: {This post is a guide to automating your finances through a checking account. It emphasizes the importance of setting up auto-pay for recurring expenses, consolidating accounts, and creating a buffer to cover unexpected bills. The post suggests that the initial overpayment is necessary to establish} \n",
      "    \n",
      "    Summary B: {The post describes how to automate payments and set up automatic deposits into a checking account for recurring expenses. It suggests that the user should set up auto-pay for all expenses, and that the user should consolidate accounts and save.\n",
      "\n",
      "It's a helpful and}\n",
      "\n",
      "    \n",
      "    Which summary is better? Respond with only <A> or <B>.\n",
      "    -------------------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(text[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c980f67-31e7-49bb-9a46-c46e0a6f61d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
