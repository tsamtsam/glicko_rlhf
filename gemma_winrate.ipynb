{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c215fd8b-9481-44d1-92c7-e1a889033a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from transformers import (\n",
    "    GPT2Model,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2PreTrainedModel,\n",
    "    GPT2Config,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    TextStreamer,\n",
    " AutoTokenizer\n",
    ")\n",
    "import random\n",
    "from typing import Dict, List\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset as HFDataset\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import datetime\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc276cc8-532a-4945-9813-4f9f52e475a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('summaries.csv')\n",
    "random.seed(40)\n",
    "labels = np.random.binomial(size=len(df), n=1, p= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd5682a9-c12e-4dc1-abcd-706ce1d56468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Data for GPT2\n",
    "text = []\n",
    "ml = 0 \n",
    "for i, row in df.iterrows():\n",
    "    # Process the prompt\n",
    "    prompt = row['Prompt']\n",
    "    index = prompt.find('Post')\n",
    "    if index != -1: #check if substring exists\n",
    "        prompt = prompt[index+5:]\n",
    "        prompt = prompt[: len(prompt)-8]\n",
    "    pre = '''Which of the following summaries does a better job of summarizing the most important points in the given forum post, without including unimportant or irrelevant details? Judge based on accuracy, coverage, and coherence.\n",
    "    Post: {''' + prompt +  '''} \n",
    "    Summary A: {''' + row['Pessimism'] + '''} \n",
    "    \n",
    "    Summary B: {''' + row['Base'] + '''}\n",
    "\n",
    "    \n",
    "    Which summary is better? Respond with only <A> or <B>.\n",
    "    -------------------------------------------------------\n",
    "    '''\n",
    "    if labels[i] == 0:\n",
    "            pre = '''Which of the following summaries does a better job of summarizing the most important points in the given forum post, without including unimportant or irrelevant details? Judge based on accuracy, coverage, and coherence.\n",
    "    Post: {''' + prompt +  '''} \n",
    "    Summary A: {''' + row['Base'] + '''} \n",
    "    \n",
    "    Summary B: {''' + row['Pessimism'] + '''}\n",
    "\n",
    "    \n",
    "    Which summary is better? Respond with only <A> or <B>.\n",
    "    -------------------------------------------------------\n",
    "    '''\n",
    "    text.append(pre)\n",
    "    ml = max(ml, len(text[-1].split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9ac5484-e58a-40eb-8d21-100e41dedd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../whome/Downloads/summ.txt', 'w') as f:\n",
    "    for line in text:\n",
    "        f.write(\"%s\\n\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c6ac2ff-da43-4280-b61f-178afa2933d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = []\n",
    "with open('../whome/Downloads/summary_choices.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Process each line here\n",
    "        l = line.strip()\n",
    "        if len(l) > 0:\n",
    "            l= l.split(' ')\n",
    "            if l[1] == 'A':\n",
    "                c1.append(1)\n",
    "            else:\n",
    "                c1.append(0)\n",
    "c2 = []\n",
    "with open('../whome/Downloads/summary_choices_b.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Process each line here\n",
    "        l = line.strip()\n",
    "        if len(l) > 0:\n",
    "            l= l.split(' ')\n",
    "            if l[1] == 'A':\n",
    "                c2.append(1)\n",
    "            else:\n",
    "                c2.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e067f15-95c9-4c60-92ed-1aadf4bd387a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.964 0.9641434262948207\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(c1)/len(c1), np.sum(c2)/len(c2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "81f5e5b2-cb8b-407f-8801-a3e24fa78845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4541832669322709\n"
     ]
    }
   ],
   "source": [
    "wr = 0 \n",
    "for v1, v2 in zip(labels, c2):\n",
    "    if v1 == v2:\n",
    "        wr += 1\n",
    "print(wr/len(c2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "42481b83-79c9-4bb1-bf6c-668beb0defc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4560260586319218"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(labels)/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "657a9c7e-7896-4dcf-9253-a7840b77e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d915f5a-bcb8-425f-b97d-c0691ec256ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#String processing\n",
    "def remove_before_char(text, char):\n",
    "  \"\"\"Removes the portion of the string before the first occurrence of the specified character.\n",
    "\n",
    "  Args:\n",
    "    text: The input string.\n",
    "    char: The character to search for.\n",
    "\n",
    "  Returns:\n",
    "    The modified string, or the original string if the character is not found.\n",
    "  \"\"\"\n",
    "  index = text.find(char)\n",
    "  if index != -1:\n",
    "    return text[index:]\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6c6d6cb-e46d-446c-9b8d-f00255989fbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m pess \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m ml \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Process the prompt\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrompt\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m     index \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Format Data for GPT2\n",
    "base = []\n",
    "pess = []\n",
    "ml = 0 \n",
    "for index, row in df.iterrows():\n",
    "    # Process the prompt\n",
    "    prompt = row['Prompt']\n",
    "    index = prompt.find('Post')\n",
    "    if index != -1: #check if substring exists\n",
    "        prompt = prompt[index+5:]\n",
    "        prompt = prompt[: len(prompt)-8]\n",
    "    \n",
    "    pre = '''Which of the following summaries does a better job of summarizing the most important points in the given forum post, without including unimportant or irrelevant details? Judge based on accuracy, coverage, and coherence.\n",
    "    Post: ''' + prompt +  ''' \n",
    "    Summary A: ''' + row['Human'] + ''' \n",
    "    Summary B: ''' \n",
    "    \n",
    "    post = '''\n",
    "    Which summary is better? Respond with only <A> or <B>.\n",
    "    _____________________________________________________________________________________________________________'''\n",
    "    base.append(pre + row['Base'] + post)\n",
    "    pess.append(pre + row['Pessimism'] + post)\n",
    "    ml = max(ml, len(base[-1].split(' ')), len(pess[-1].split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf6bc073-da83-4664-9993-559bf8ba79ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaf9e60c-fd63-4562-858b-fa2ce98ab591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load GPT-2 Large model and tokenizer\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\n",
    "# model = GPT2LMHeadModel.from_pretrained('gpt2-large')\n",
    "# # Add padding token if missing\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29bf21df-fd51-4fd8-a926-8fe99969850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 8\n",
    "max_seq_length = 800 + max_new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cd8c9dc-cf1b-4000-91cb-3e2955ae2554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.7: Fast Gemma3 patching. Transformers: 4.51.3. vLLM: 0.8.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.999 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gemma3ForCausalLM(\n",
       "  (model): Gemma3TextModel(\n",
       "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 1152, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear(in_features=1152, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1152, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "          (up_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "          (down_proj): Linear(in_features=6912, out_features=1152, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "    (rotary_emb): Gemma3RotaryEmbedding()\n",
       "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1152, out_features=262144, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in Gemma3 to evaluate if the generated summary is better than the human one\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-1b-it\",\n",
    "    max_seq_length = max_seq_length, # Choose any for long context!\n",
    "    load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54877b07-9ad5-426d-865a-7991d82b8946",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = base[:5] + pess[:5]\n",
    "inputs = tokenizer(\n",
    "    text, # Process a list of prompts\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True, # Pad to the longest sequence in the batch\n",
    "    truncation=True,\n",
    "    max_length=max_seq_length - max_new_tokens # Make space for generated text\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad(): # Disable gradient calculation for inference\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        do_sample=True,      # Use sampling\n",
    "        temperature=0.25,     # Lower temperature for less randomness\n",
    "        top_p=.9,           # Nucleus sampling\n",
    "        num_return_sequences=1,\n",
    "        min_length = 1,\n",
    "    )\n",
    "generated_summaries = []\n",
    "# Decode each summary in the batch\n",
    "# outputs contains the full sequence (prompt + summary)\n",
    "# We need to slice off the prompt part for each generated summary\n",
    "input_ids_length = inputs.input_ids.shape[1] # Length of the tokenized input prompts (padded)\n",
    "for i in range(outputs.shape[0]): # Iterate through each item in the batch\n",
    "    \n",
    "    summary_ids = outputs[i][input_ids_length:]\n",
    "    summary = tokenizer.decode(summary_ids, skip_special_tokens=True)\n",
    "    generated_summaries.append(summary.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65f8cf4a-f702-46e4-a5d3-5e8035cf9490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_first(text):\n",
    "    one_index = text.find(\"A\")\n",
    "    two_index = text.find(\"B\")\n",
    "\n",
    "    if one_index == -1 and two_index == -1:\n",
    "        return -1\n",
    "    elif one_index == -1:\n",
    "        return 2\n",
    "    elif two_index == -1:\n",
    "        return 1\n",
    "    elif one_index < two_index:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7d9208a-f308-4821-9be7-7d7a32c9891b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for a in generated_summaries:\n",
    "    print(check_first(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d33db24f-5501-4a04-9b43-87e4b0b46691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<A> My husband is lazy', '<A> <B>', '<A>\\n<B>', '<A>\\n<B>', '<A> <B>', '<A>\\n<B>', '<A>\\n<B>', '<A> is better.', '<A>\\n    <', '<A>\\n<B>']\n"
     ]
    }
   ],
   "source": [
    "print(generated_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "943d6d9f-f61e-469c-97ef-3627095cc5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = base[:100] + pess[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aceb75b5-fc8e-4776-95cd-ba6c6ed2a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../whome/Downloads/summ.txt', 'w') as f:\n",
    "    for line in text:\n",
    "        f.write(\"%s\\n\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5ceefba-57e4-4298-890d-5da291e1db2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which of the following summaries does a better job of summarizing the most important points in the given forum post, without including unimportant or irrelevant details? Judge based on accuracy, coverage, and coherence.\n",
      "    Post: {\n",
      "So my fiancé was grocery shopping at the commissary and cashier asked him if he wanted to get what he understood was a points card. He even asked to make sure it wasn't a credit card because we are about to close on a house in a couple of months and do not want anything to affect our credit. She said it wasn't and he signed up. \n",
      "\n",
      "He didn't think much of putting his SSN in because in the military it's your identity number and his rank and all showed up when he put it in, and then he discovered it was a CREDIT CARD. Is there anything we can do to dispute this. It's already popped up on his credit report and he doesn't want another credit card..\n",
      "\n",
      "Summarize the post in two se} \n",
      "    Summary A: {The fiancé signed up for a credit card through the commissary cashier, a point card, which he initially believed was a secure way to avoid affecting his credit. However, he later discovered the card was a credit card, leading to a potential problem with his credit report} \n",
      "    \n",
      "    Summary B: {This man signed up for a points card at the commissary, seemingly unaware of the credit card implications, and his fiancé insisted it wasn't a credit card. He's now facing a potential credit card issue and needs to investigate how to dispute this.}\n",
      "\n",
      "    \n",
      "    Which summary is better? Respond with only <A> or <B>.\n",
      "    -------------------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c980f67-31e7-49bb-9a46-c46e0a6f61d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
