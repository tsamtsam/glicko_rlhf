{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b7ca308-e42d-41a8-967e-0b198fcd8d45",
   "metadata": {},
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth vllm\n",
    "else:\n",
    "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
    "    !pip install --no-deps unsloth vllm\n",
    "# Install latest Hugging Face for Gemma-3!\n",
    "!pip install --no-deps git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f02461ce-1967-4f5d-bb0b-f8d478d445af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-23 21:39:30 [__init__.py:256] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from transformers import TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48b13ed9-e9ac-45ce-a90d-c716aabf31ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.17: Fast Gemma3 patching. Transformers: 4.50.0.dev0. vLLM: 0.8.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.999 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 1024\n",
    "\n",
    "fourbit_models = [\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "\n",
    "    # Other popular models!\n",
    "    \"unsloth/Llama-3.1-8B\",\n",
    "    \"unsloth/Llama-3.2-3B\",\n",
    "    \"unsloth/Llama-3.3-70B\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
    "    \"unsloth/Phi-4\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-1b-it\",\n",
    "    max_seq_length = max_seq_length, # Choose any for long context!\n",
    "    load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a687cd55-0b1c-40c9-8d40-5fec8ff666a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # Turn off for just text!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = True,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
    "\n",
    "    r = 8,           # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha = 8,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be4a50c7-30c1-4366-98ee-b10873dc2c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 7473\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"openai/gsm8k\", \"main\", split = \"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63d9cf00-3e96-49f7-9dba-c1000b1c2d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       " 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "876bb046-860e-4917-aaed-76fddec4afa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'72'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_hash_answer(text):\n",
    "    if \"####\" not in text: return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "extract_hash_answer(dataset[0][\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "289c7f14-a7d6-4558-a239-5327854bb8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reasoning_start = \"<start_working_out>\"\n",
    "reasoning_end   = \"<end_working_out>\"\n",
    "solution_start = \"<SOLUTION>\"\n",
    "solution_end = \"</SOLUTION>\"\n",
    "\n",
    "system_prompt = \\\n",
    "f\"\"\"You are given a problem.\n",
    "Think about the problem and provide your working out.\n",
    "Place it between {reasoning_start} and {reasoning_end}.\n",
    "Then, provide your solution between {solution_start}{solution_end}\"\"\"\n",
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5ff5df2-ac21-4032-8aaf-957db4207043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       " 'answer': '72',\n",
       " 'prompt': [{'content': 'You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION>',\n",
       "   'role': 'system'},\n",
       "  {'content': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       "   'role': 'user'}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: {\n",
    "    \"prompt\" : [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": x[\"question\"]},\n",
    "    ],\n",
    "    \"answer\": extract_hash_answer(x[\"answer\"]),\n",
    "})\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ffe9cac-ff8e-407d-bffb-92b751d15a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_format = re.compile(\n",
    "    rf\"^[\\s]{{0,}}\"\\\n",
    "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\\\n",
    "    rf\"{solution_start}(.+?){solution_end}\"\\\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd20a5a3-9605-42e6-939a-f28ca0e603c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 71), match='<start_working_out>Let me think!<end_working_out>>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_format.search(\n",
    "    \"<start_working_out>Let me think!<end_working_out>\"\\\n",
    "    \"<SOLUTION>2</SOLUTION>\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd49d9ac-a716-47ba-8c6b-700962ccb455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_format_exactly(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # Match if format is seen exactly!\n",
    "        if match_format.search(response) is not None: score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a51c9e7-3652-4170-9ba7-c195c3895760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_format_approximately(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # Count how many keywords are seen - we penalize if too many!\n",
    "        # If we see 1, then plus some points!\n",
    "        score += 0.5 if response.count(reasoning_start) == 1 else -0.5\n",
    "        score += 0.5 if response.count(reasoning_end)   == 1 else -0.5\n",
    "        score += 0.5 if response.count(solution_start)  == 1 else -0.5\n",
    "        score += 0.5 if response.count(solution_end)    == 1 else -0.5\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5f8e5b2-439c-4f1b-be50-dc15b0c0ae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_format.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        score = 0\n",
    "        if guess is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        # Correct answer gets 3 points!\n",
    "        if guess == true_answer:\n",
    "            score += 3.0\n",
    "        # Match if spaces are seen\n",
    "        elif guess.strip() == true_answer.strip():\n",
    "            score += 1.5\n",
    "        else:\n",
    "            # We also reward it if the answer is close via ratios!\n",
    "            # Ie if the answer is within some range, reward it!\n",
    "            try:\n",
    "                ratio = float(guess) / float(true_answer)\n",
    "                if   ratio >= 0.9 and ratio <= 1.1: score += 0.5\n",
    "                elif ratio >= 0.8 and ratio <= 1.2: score += 0.25\n",
    "                else: score -= 1.0 # Penalize wrong answers\n",
    "            except:\n",
    "                score -= 0.5 # Penalize\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66cf274c-f16c-4434-8118-40f7996a7632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.34']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_numbers = re.compile(\n",
    "    rf\"{solution_start}.*?([\\d\\.]{{1,}})\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "match_numbers.findall(\"<SOLUTION>  0.34  </SOLUTION>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77049530-5fa4-4833-8185-01a097e212c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_numbers(prompts, completions, answer, **kwargs):\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_numbers.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    print('*'*20, f\"Question:\\n{question}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        if guess is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        # Convert to numbers\n",
    "        try:\n",
    "            true_answer = float(true_answer.strip())\n",
    "            guess       = float(guess.strip())\n",
    "            scores.append(1.5 if guess == true_answer else 0.0)\n",
    "        except:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7faff3e4-4705-40ea-bc46-9fd9c8200176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 1 to the `num_generations` of 2\n"
     ]
    }
   ],
   "source": [
    "max_prompt_length = 256\n",
    "training_args = GRPOConfig(\n",
    "    learning_rate = 5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"adamw_torch_fused\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
    "    num_generations = 2, # Decrease if out of memory\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_seq_length - max_prompt_length,\n",
    "    num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    max_steps = 25,\n",
    "    save_steps = 50,\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir = \"outputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50f5eed9-aa10-454d-a533-99f69242b00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_answer,\n",
    "        check_numbers,\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d0ff3a8-ef89-4c32-ac49-da232380d71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 7,473 | Num Epochs = 1 | Total steps = 25\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 1 x 1) = 2\n",
      " \"-____-\"     Trainable parameters = 6,522,880/1,006,408,832 (0.65% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Question:\n",
      "A concert ticket costs $40. Mr. Benson bought 12 tickets and received a 5% discount for every ticket bought that exceeds 10. How much did Mr. Benson pay in all? \n",
      "Answer:\n",
      "476 \n",
      "Response:\n",
      "<start_working_out>\n",
      "Let the cost of each ticket be $40.\n",
      "Mr. Benson bought 12 tickets.\n",
      "The number of tickets that exceed 10 is 12 - 10 = 2 tickets.\n",
      "For each of these 2 tickets, he receives a 5% discount.\n",
      "Discount per ticket = 5% of $40 = 0.05 \\times 40 = $2.\n",
      "Price of each ticket with discount = $40 - $2 = $38.\n",
      "Total cost of 2 tickets with discount = 2 \\times $38 = $76.\n",
      "Total cost of 12 tickets = 12 \\times 40 = $480.\n",
      "Total cost of discounts = 2 \\times $2 = $4.\n",
      "Total cost of tickets = $76 + $4 = $80.\n",
      "Alternatively, the total cost of 12 tickets without discount is $12 \\times 40 = $480.\n",
      "He receives a 5% discount for every ticket bought that exceeds 10. So for the 2 tickets, the discount is $2 \\times 0.05 = $0.10.\n",
      "Therefore, the price of the 2 tickets is $480 - $0.10 \\times 12 = $480 - $1.20 = $478.80.\n",
      "But this method is incorrect.\n",
      "Let $N$ be the number of tickets bought, so $N = 12$.\n",
      "The ticket cost is $40$.\n",
      "The discount for every ticket exceeding 10 is 5%.\n",
      "The number of tickets exceeding 10 is $12 - 10 = 2$.\n",
      "The discount for each of these 2 tickets is $5\\%$ of $40$, which is $0.05 \\times 40 = 2$.\n",
      "The price of each of these 2 tickets is $40 - 2 = 38$.\n",
      "The total cost of the 2 tickets is $2 \\times 38 = 76$.\n",
      "The total cost of the 12 tickets is $12 \\times 40 = 480$.\n",
      "The total discount is $2 \\times 2 = 4$.\n",
      "The total cost is $480 - 4 = 476$.\n",
      "However, the discount is applied to the price of each ticket. So the price of each ticket with discount is $40 \\times (1 - 0.05) = 40 \\times 0.95 = 38$.\n",
      "The total cost of 12 tickets with discount is $12 \\times 38 = 456$.\n",
      "The total cost of 12 tickets is $12 \\times 40 = 480$.\n",
      "The total discount is $12 \\times 2 \\times 0.05 = 12 \\times 0.1 = 1.2$.\n",
      "The total cost is $480 - 1.2 = 478.8$.\n",
      "But we are asked for the amount Mr. Benson paid in all.\n",
      "Let $C$ be the cost of each ticket. $C = 40$.\n",
      "Let $N$ be the number of tickets bought. $N = 12$.\n",
      "Mr. Benson received a 5% discount for every ticket bought that exceeds 10. So, for each ticket that exceeds 10, he gets a 5 \n",
      "Extracted:\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3/25 00:43 < 15:56, 0.02 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completion_length</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / match_format_exactly</th>\n",
       "      <th>rewards / match_format_approximately</th>\n",
       "      <th>rewards / check_answer</th>\n",
       "      <th>rewards / check_numbers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n",
      "******************** Question:\n",
      "Jane is trying to decide whether to buy a house or a trailer. A house costs $480,000 and a trailer costs $120,000. Each loan will be paid in monthly installments over 20 years. How much more is the monthly payment on the house compared to the trailer? \n",
      "Answer:\n",
      "1500 \n",
      "Response:\n",
      "<start_working_out>\n",
      "Let's calculate the monthly payment for each option.\n",
      "House cost: $480,000\n",
      "Monthly payment for house: $480,000 / 12 = $40,000 per month\n",
      "Trailer cost: $120,000\n",
      "Monthly payment for trailer: $120,000 / 12 = $10,000 per month\n",
      "\n",
      "The difference in monthly payment between the house and the trailer is:\n",
      "$40,000 - $10,000 = $30,000\n",
      "\n",
      "Therefore, the monthly payment on the house is $30,000 more than the monthly payment on the trailer.\n",
      "\n",
      "<SOLUTION>30000 \n",
      "Extracted:\n",
      "30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ExactWeakKeyDictionary.__setitem__.<locals>.<lambda> at 0x7fa9f9895f30>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tsam/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 589, in <lambda>\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0622da-8ba0-4994-838f-5cb63c35b6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\",   \"content\": \"What is the sqrt of 08101 + 93^2?\"},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    tokenize = False,\n",
    ")\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 128, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a8a8e2-54cb-419d-b8a6-bb9ec6fb0bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a local directory\n",
    "output_path = \"gemma_grpo_math\"\n",
    "trainer.save_model(output_path)\n",
    "print(f\"Model saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46f516a0-2e09-4908-a47a-3f7815eb033a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.17: Fast Gemma3 patching. Transformers: 4.50.0.dev0. vLLM: 0.8.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.999 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Option 1: Load from local directory\n",
    "model_path = \"gemma_grpo_math\"  # Local directory where model was saved\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = model_path,\n",
    "    max_seq_length = 1024,\n",
    "    load_in_4bit = False,\n",
    "    load_in_8bit = False,\n",
    ")\n",
    "\n",
    "# Option 2: Load from Hugging Face Hub\n",
    "# model, tokenizer = FastModel.from_pretrained(\n",
    "#     model_name = \"your-username/gemma-grpo-math\",  # Your HF model name\n",
    "#     max_seq_length = 1024,\n",
    "#     load_in_4bit = False,\n",
    "#     load_in_8bit = False,\n",
    "#     token = \"hf_...\"  # Your HF token if model is private\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b7a1452-d13a-4c3d-8704-ea1f50522354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating response with fine-tuned model:\n",
      "<start_working_out>\n",
      "Let J be the number of apples John has.\n",
      "Let M be the number of apples Mary has.\n",
      "We are given that John has 5 apples, so J = 5.\n",
      "We are given that Mary has 3 more apples than John, so M = J + 3.\n",
      "Substituting J = 5, we have M = 5 + 3 = 8.\n",
      "The total number of apples they have is J + M = 5 + 8 = 13.\n",
      "<SOLUTION>13<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the loaded model\n",
    "reasoning_start = \"<start_working_out>\"\n",
    "reasoning_end   = \"<end_working_out>\"\n",
    "solution_start = \"<SOLUTION>\"\n",
    "solution_end = \"</SOLUTION>\"\n",
    "\n",
    "system_prompt = f\"\"\"You are given a problem.\n",
    "Think about the problem and provide your working out.\n",
    "Place it between {reasoning_start} and {reasoning_end}.\n",
    "Then, provide your solution between {solution_start}{solution_end}\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\",   \"content\": \"If John has 5 apples and Mary has 3 more apples than John, how many apples do they have in total?\"},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,\n",
    "    tokenize = False,\n",
    ")\n",
    "\n",
    "print(\"Generating response with fine-tuned model:\")\n",
    "output = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 256,\n",
    "    temperature = 0.7, \n",
    "    top_p = 0.95, \n",
    "    top_k = 40,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8cbe9f-3d98-4573-80b8-ba9ba93252cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
