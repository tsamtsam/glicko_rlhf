{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdce5fd6-519c-46d7-a9ec-1e362de8215a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-24 00:14:38 [__init__.py:256] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    TextStreamer\n",
    ")\n",
    "from typing import Dict, List\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset as HFDataset\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from unsloth import is_bfloat16_supported\n",
    "# One must patch the DPO Trainer first!\n",
    "from unsloth import PatchDPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f8167d-ba40-4204-b481-3fd8edf300a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "    !pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a47cb8c-5714-48cf-94a0-9b305454d227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cc01551-8caa-42da-ba8e-b3b8fbb7571b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90e4778d2554efa942fa9dc684e4dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2504a5adc0341e5954b34d9439a98a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831d820f30ba429c8a546450dc4e4eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['post_id', 'domain', 'upvote_ratio', 'history', 'c_root_id_A', 'c_root_id_B', 'created_at_utc_A', 'created_at_utc_B', 'score_A', 'score_B', 'human_ref_A', 'human_ref_B', 'labels', 'seconds_difference', 'score_ratio'],\n",
       "    num_rows: 3487\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"stanfordnlp/shp\",  split='train[:1%]')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe3d4acd-8282-491d-8919-49ef42fe9934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'post_id': 'himc90',\n",
       " 'domain': 'askacademia_train',\n",
       " 'upvote_ratio': 0.99,\n",
       " 'history': 'In an interview right before receiving the 2013 Nobel prize in physics, Peter Higgs stated that he wouldn\\'t be able to get an academic job today, because he wouldn\\'t be regarded as productive enough. > By the time he retired in 1996, he was uncomfortable with the new academic culture. \"After I retired it was quite a long time before I went back to my department. I thought I was well out of it. It wasn\\'t my way of doing things any more. Today I wouldn\\'t get an academic job. It\\'s as simple as that. I don\\'t think I would be regarded as productive enough.\"  Another interesting quote from the article is the following:  > He doubts a similar breakthrough could be achieved in today\\'s academic culture, because of the expectations on academics to collaborate and keep churning out papers. He said: \"It\\'s difficult to imagine how I would ever have enough peace and quiet in the present sort of climate to do what I did in 1964.\"  Source (the whole article is pretty interesting): http://theguardian.com/science/2013/dec/06/peter-higgs-boson-academic-system',\n",
       " 'c_root_id_A': 'fwhnqat',\n",
       " 'c_root_id_B': 'fwhp8d4',\n",
       " 'created_at_utc_A': 1593535113,\n",
       " 'created_at_utc_B': 1593535824,\n",
       " 'score_A': 52,\n",
       " 'score_B': 54,\n",
       " 'human_ref_A': 'Currently wrapping up my PhD. There is a stark difference in work balance life between students in my lab who are focused on industry and those focused on academia. The ones in academia feel an immense stress to get high level publications (some staying 8+ years to try to push something into nature/science). The competition has become cut throat. This is a trend not just in America but in Europe, Asia and middle east. International graduate students tell me in China go back 20 years, having any ACS publication from american university is enough to get professorship. Now you better come stacked with publications and at least one nature/science. American universities are even more competitive. How many publications, how many conferences, how many patents...',\n",
       " 'human_ref_B': 'Itâ€™s ironic to me that research has shown that productivity isnâ€™t all itâ€™s cracked up to be yet here we are.',\n",
       " 'labels': 0,\n",
       " 'seconds_difference': 711.0,\n",
       " 'score_ratio': 1.0384615385,\n",
       " 'prompt': 'In an interview right before receiving the 2013 Nobel prize in physics, Peter Higgs stated that he wouldn\\'t be able to get an academic job today, because he wouldn\\'t be regarded as productive enough. > By the time he retired in 1996, he was uncomfortable with the new academic culture. \"After I retired it was quite a long time before I went back to my department. I thought I was well out of it. It wasn\\'t my way of doing things any more. Today I wouldn\\'t get an academic job. It\\'s as simple as that. I don\\'t think I would be regarded as productive enough.\"  Another interesting quote from the article is the following:  > He doubts a similar breakthrough could be achieved in today\\'s academic culture, because of the expectations on academics to collaborate and keep churning out papers. He said: \"It\\'s difficult to imagine how I would ever have enough peace and quiet in the present sort of climate to do what I did in 1964.\"  Source (the whole article is pretty interesting): http://theguardian.com/science/2013/dec/06/peter-higgs-boson-academic-system',\n",
       " 'chosen': 'Itâ€™s ironic to me that research has shown that productivity isnâ€™t all itâ€™s cracked up to be yet here we are.',\n",
       " 'rejected': 'Currently wrapping up my PhD. There is a stark difference in work balance life between students in my lab who are focused on industry and those focused on academia. The ones in academia feel an immense stress to get high level publications (some staying 8+ years to try to push something into nature/science). The competition has become cut throat. This is a trend not just in America but in Europe, Asia and middle east. International graduate students tell me in China go back 20 years, having any ACS publication from american university is enough to get professorship. Now you better come stacked with publications and at least one nature/science. American universities are even more competitive. How many publications, how many conferences, how many patents...'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Format Dataset for GRPO Trainer\n",
    "dataset = dataset.map(lambda x: {\n",
    "    \"prompt\" : x['history'],\n",
    "    \"chosen\": x['human_ref_A'] if x['labels'] == 1 else x['human_ref_B'],\n",
    "    \"rejected\": x['human_ref_A'] if x['labels'] == 0 else x['human_ref_B']\n",
    "})\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6130d1d9-a2fa-4080-a6eb-9abbe89e508e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.17: Fast Gemma3 patching. Transformers: 4.50.0.dev0. vLLM: 0.8.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.999 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    }
   ],
   "source": [
    "# Load in Gemma 3\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "max_prompt_length = 256\n",
    "max_seq_length = 1024\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-1b-it\",\n",
    "    max_seq_length = max_seq_length, # Choose any for long context!\n",
    "    load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1acb7b4c-56b4-4e4c-a75c-c1b37ed9b490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # Turn off for just text!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = True,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
    "\n",
    "    r = 8,           # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha = 8,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "296e2aee-65da-4129-b6f5-b8faaee4c292",
   "metadata": {},
   "outputs": [],
   "source": [
    "PatchDPOTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6b92bc-5a15-479c-affc-cdfed3f62fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f7af463cc9e442e9bea5b19b0547c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset (num_proc=12):   0%|          | 0/3487 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23aa8b3039584b5e875b9e6dde921caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=12):   0%|          | 0/3487 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dpo_trainer = DPOTrainer(\n",
    "    model = model,\n",
    "    ref_model = None,\n",
    "    args = DPOConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 2,\n",
    "        warmup_ratio = 0.1,\n",
    "        num_train_epochs = 1,\n",
    "        learning_rate = 5e-6,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.0,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 42,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    "    beta = 0.1,\n",
    "    train_dataset = dataset,\n",
    "    # eval_dataset = raw_datasets[\"test\"],\n",
    "    tokenizer = tokenizer,\n",
    "    max_length = max_seq_length,\n",
    "    max_prompt_length = max_prompt_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610a3103-5ed9-4847-b46d-775cca162a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c378fe0-f0a6-485f-905f-a350f854503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a local directory\n",
    "output_path = \"gemma_dpo\"\n",
    "trainer.save_model(output_path)\n",
    "print(f\"Model saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f536a3-8e2b-482a-97a0-6984837dc300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5d9350-b6ab-4b31-a776-caeda64f6b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d55c66a-4f9f-4a0b-b897-b1f9a1bba818",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
